{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_hypersphere(d):\n",
    "    # sample a point uniformly from d-1 sphere\n",
    "    x = np.random.randn(d)\n",
    "    x /= np.linalg.norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_sample(D, c, X):\n",
    "    y = X @ (D @ c) # no noise added\n",
    "    return y\n",
    "def X_sample(n, d, distribution_sample):\n",
    "    # a n \\times d matrix with each row sampled according to function distribution_sample\n",
    "    X = []\n",
    "    for i in range(n):\n",
    "        X.append(distribution_sample(d))\n",
    "    return np.array(X)\n",
    "\n",
    "def c_sample(k, distribution_sample):\n",
    "    # a k dimensional vector sampled according to distribution_sample\n",
    "    return distribution_sample(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_c_loss_gradient_without_query(X, y, D, lambd=0.):\n",
    "    \n",
    "    k = D.shape[1] \n",
    "    S = np.linalg.inv((D.T@X.T)@(X@D) + (lambd * np.eye(k, k)))\n",
    "    XTy = (X.T) @ y\n",
    "    DTXTy = D.T @ XTy\n",
    "    c_solved = S @ (DTXTy)\n",
    "    pred = X @ (D @ c_solved)\n",
    "    \n",
    "    S_1 = (pred - y)\n",
    "    S_2 = X.T @ S_1\n",
    "    S_3 = S @ D.T @ X.T @ S_1\n",
    "    \n",
    "    functionValue = (np.linalg.norm(t_3) ** 2)\n",
    "    gradient = 2 * (np.multiply.outer(S_2, c_solved))\\\n",
    "                - 2* ((np.multiply.outer(pred.T @ X.T, S_3)) + (np.multiply.outer((X.T @ (X @ (D @ (S @ (D.T @ S_2))))), c_solved)))\\\n",
    "                + (2 * np.multiply.outer(XTy, S_3))\n",
    "\n",
    "    return c_solved, functionValue, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_c_loss_gradient_without_query(X, y, D, lambd=0.):\n",
    "    A = X @ D\n",
    "    ATA = A.T @ A\n",
    "    ATA_inv = np.linalg.inv(ATA + np.eye(D.shape[1]) * lambd)\n",
    "    XTy = X.T @ y\n",
    "    DTXTy = D.T @ XTy\n",
    "    c_solved = ATA_inv @ DTXTy\n",
    "    loss = np.sum(np.square(y)) - np.inner(DTXTy, c_solved)\n",
    "    gradient = -2 * np.outer(XTy, c_solved)+ 2 * np.outer((X.T @ (A @ c_solved)), c_solved.T)\n",
    "    return c_solved, loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D_true, D_init, X_pool, c_pool, k_actual, lambd, lr, num_iterations_train, test_frequency):\n",
    "    D = D_init\n",
    "    alpha = 0.05\n",
    "    num_iterations = num_iterations_train\n",
    "    losses = []\n",
    "    avg_train_loss = []\n",
    "    sum_train_loss = 0.0\n",
    "    avg_test_loss = []\n",
    "\n",
    "    for iteration in tqdm(range(num_iterations)):\n",
    "        X = X_pool[np.random.choice(len(X_pool))]\n",
    "        c = c_pool[np.random.choice(len(c_pool))]\n",
    "        y = y_sample(D=D_true, c=c, X=X)\n",
    "        c_solved, loss, gradient = task_c_loss_gradient_without_query(X, y, D, lambd)\n",
    "        losses.append(loss)\n",
    "        D = D - alpha * gradient # update the loss\n",
    "        sum_train_loss += loss\n",
    "        avg_train_loss.append(sum_train_loss / (iteration + 1))\n",
    "\n",
    "        if iteration % test_frequency == 0: # test evaluation\n",
    "            sum_test_loss = 0.0\n",
    "            for i in range(2000):\n",
    "                X = X_sample(n=n, d=d, distribution_sample=uniform_hypersphere)\n",
    "                c = c_sample(k_actual, distribution_sample=uniform_hypersphere)\n",
    "                y = y_sample(D=D_true, c=c, X=X)\n",
    "                c_solved, loss, gradient = task_c_loss_gradient_without_query(X, y, D, lambd)\n",
    "                sum_test_loss += loss\n",
    "            avg_test_loss.append(sum_test_loss / 1000)\n",
    "            \n",
    "    return avg_train_loss, avg_test_loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 521/1000 [01:25<01:16,  6.22it/s]"
     ]
    }
   ],
   "source": [
    "########### THIS IS A TYPICAL RUN ###########\n",
    "\n",
    "# Set parameters\n",
    "n = 5 # number of examples per task\n",
    "d = 10 # ambient dimension\n",
    "k = 100 # subspace dimension we learn (used in prediction)\n",
    "k_actual = 2 # actual subspace dimension of the weights (used in data generation)  \n",
    "T = 100 # total number of tasks\n",
    "M = 10000 # total number of X matrix of shape n \\times d\n",
    "m = 1 # totatl number of fixML's matrix X\n",
    "lr = 0.1 # learning rate for learning D\n",
    "num_iterations_train = 1000 # no of train_iterations\n",
    "test_frequency = 10 # no of iterations after which we test\n",
    "lambd = 10. # inner loop regularizer\n",
    "\n",
    "# Get Dataset\n",
    "X_pool = [X_sample(n=n, d=d, distribution_sample=uniform_hypersphere) for i in range(M)]\n",
    "X_fix_pool = X_pool[:m]\n",
    "c_pool = [c_sample(k_actual, distribution_sample=uniform_hypersphere) for i in range(T)]\n",
    "\n",
    "\n",
    "# obtain meta-learning and fix-meta learning solns\n",
    "D_true = np.random.randn(d, k_actual)\n",
    "D_init = np.random.randn(d, k)\n",
    "metal_avg_train_loss, metal_avg_test_loss = train(D_true, D_init, X_pool, c_pool, k_actual, lambd, lr, num_iterations_train, test_frequency)\n",
    "fix_metal_avg_train_loss, fix_metal_avg_test_loss = train(D_true, D_init, X_fix_pool, c_pool, k_actual, lambd, lr, num_iterations_train, test_frequency)\n",
    "\n",
    "# plots\n",
    "plt.plot(list(range(num_iterations_train)), metal_avg_train_loss, label='ML')\n",
    "plt.plot(list(range(num_iterations_train)), fix_metal_avg_train_loss, label='FIX-ML')\n",
    "plt.title('train')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(num_iterations_train // test_frequency)), metal_avg_test_loss, label='ML')\n",
    "plt.plot(list(range(num_iterations_train // test_frequency)), fix_metal_avg_test_loss, label='FIX-ML')\n",
    "plt.title('test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
